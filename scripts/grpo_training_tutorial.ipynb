{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve Math Reasoning by RLVR\n",
    "\n",
    "This notebook utilizes [Tinker](https://thinkingmachines.ai/tinker/) by Thinking Machines Lab to implement **Group Relative Policy Optimization (GRPO)** for math reasoning on the GSM8K dataset, following the RL section of [Stanford CS336 Assignment 5](https://stanford-cs336.github.io/spring2025/assignments/5/). To learn the theory fundamentals of RL, we recommend checking out Stanford CS336 on Youtube, from [lecture 15 to lecture 17](https://www.youtube.com/watch?v=Dfu7vC9jo4w&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=15).\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "- **On-Policy GRPO**: One training step per sampling from the old policy.\n",
    "- **Off-Policy GRPO**: Multiple training steps per sampling from the old policy.\n",
    "\n",
    "We also provide tracking based on Weights & Biases so that you can visualize the training process, and check\n",
    "the training result easily.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**GRPO (Group Relative Policy Optimization)**:\n",
    "- Sample multiple responses per question (group_size=8)\n",
    "- Normalize rewards within each group\n",
    "- Advantages = (reward - group_mean) / (group_std + eps)\n",
    "- Use PPO-style clipping to prevent excessive policy updates\n",
    "\n",
    "**On-Policy vs Off-Policy**:\n",
    "- **On-policy** (`epochs_per_rollout_batch=1`): One gradient step per rollout batch\n",
    "- **Off-policy** (`epochs_per_rollout_batch>1`): Multiple gradient steps per rollout batch\n",
    "  - Freezes old_log_probs from sampling policy πθ_old\n",
    "  - Computes new_log_probs from current policy πθ at each gradient step\n",
    "  - Ratio πθ/πθ_old measures policy divergence\n",
    "\n",
    "## References\n",
    "\n",
    "- **DeepSeekMath**: https://arxiv.org/abs/2402.03300\n",
    "- **DeepSeek-R1**: https://arxiv.org/abs/2501.12948  \n",
    "- **Tinker Docs**: https://tinker-docs.thinkingmachines.ai/\n",
    "- **GSM8K Dataset**: https://huggingface.co/datasets/openai/gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports\n",
    "\n",
    "### Why Tinker?\n",
    "\n",
    "Assume we are building the GRPO training loop with PyTorch or JAX, we need to handle\n",
    "\n",
    "- FSDP/DeepSpeed for model sharding\n",
    "- Custom data loaders for distributed sampling\n",
    "- Apply LoRA to the model\n",
    "- Apply mixed precision to the model\n",
    "- GPU cluster setup and management\n",
    "- Manage distributed checkpoints\n",
    "- ...\n",
    "\n",
    "With Tinker, we only need to focus on defining the correct loss objects, and the rest of the training/inference\n",
    "hassle is taken care of automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU tinker tinker_cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import chz\n",
    "import datasets\n",
    "import tinker\n",
    "import torch\n",
    "from tinker.types.tensor_data import TensorData\n",
    "from tinker_cookbook import checkpoint_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Helper Function: Group Normalization\n",
    "\n",
    "### Why Group Normalization?\n",
    "GRPO normalizes rewards **within each group** (responses to the same question):\n",
    "```\n",
    "advantage = (reward - mean(group_rewards)) / (std(group_rewards) + eps)\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- **Relative comparison**: \"This response is better than average for this question\"\n",
    "- **Variance reduction**: Less sensitive to reward scale\n",
    "- **Credit assignment**: Rewards differences, not absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def run_compute_group_normalized_rewards(\n",
    "    reward_fn: Callable,\n",
    "    rollout_responses: list[str],\n",
    "    repeated_ground_truths: list[str],\n",
    "    group_size: int,\n",
    "    advantage_eps: float,\n",
    "    normalize_by_std: bool,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute group-normalized rewards for GRPO.\n",
    "    \n",
    "    From DeepSeekMath: https://arxiv.org/abs/2402.03300\n",
    "    \"\"\"\n",
    "    # Compute raw rewards\n",
    "    rollout_batch_size = len(rollout_responses)\n",
    "    raw_rewards_list = []\n",
    "\n",
    "    for response, ground_truth in zip(rollout_responses, repeated_ground_truths):\n",
    "        reward_dict = reward_fn(response, ground_truth)\n",
    "        raw_rewards_list.append(reward_dict[\"reward\"])\n",
    "\n",
    "    # Convert to tensor\n",
    "    raw_rewards = torch.tensor(raw_rewards_list, dtype=torch.float32)\n",
    "\n",
    "    # Reshape into groups: (n_groups, group_size)\n",
    "    n_groups = rollout_batch_size // group_size\n",
    "    rewards_grouped = raw_rewards.view(n_groups, group_size)\n",
    "\n",
    "    # Compute mean for each group\n",
    "    group_means = rewards_grouped.mean(dim=1, keepdim=True)\n",
    "\n",
    "    if normalize_by_std:\n",
    "        # GRPO: normalize by std\n",
    "        # A^(i) = (r^(i) - mean) / (std + eps)\n",
    "        group_stds = rewards_grouped.std(dim=1, keepdim=True)\n",
    "        advantages_grouped = (rewards_grouped - group_means) / (group_stds + advantage_eps)\n",
    "    else:\n",
    "        # Dr. GRPO: don't normalize by std\n",
    "        # A^(i) = r^(i) - mean\n",
    "        advantages_grouped = rewards_grouped - group_means\n",
    "\n",
    "    # Flatten back\n",
    "    advantages = advantages_grouped.view(-1)\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        \"mean_reward\": raw_rewards.mean().item(),\n",
    "        \"std_reward\": raw_rewards.std().item(),\n",
    "        \"min_reward\": raw_rewards.min().item(),\n",
    "        \"max_reward\": raw_rewards.max().item(),\n",
    "    }\n",
    "\n",
    "    return advantages, raw_rewards, metadata\n",
    "\n",
    "print(\"✓ Group normalization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "We are mostly using the same hyperparameters as provided in the [assignment](https://github.com/stanford-cs336/assignment5-alignment) page 27, with some slight changes to accomodate the Tinker spec, e.g., using Qwen3-8B instad of\n",
    "Qwen3-1.5B-Math because Tinker doesn't support the latter one.\n",
    "\n",
    "**Model & Training:**\n",
    "- `model_name`: Base LLM (Qwen3-8B)\n",
    "- `lora_rank`: LoRA adapter rank (32)\n",
    "- `learning_rate`: 1e-5\n",
    "\n",
    "**GRPO Specific:**\n",
    "- `rollout_batch_size`: Total rollouts per sampling step (256 = 32 inputs × 8 samples)\n",
    "- `group_size`: Samples per input for group normalization (8)\n",
    "- `cliprange`: PPO clip range for stability (0.2)\n",
    "\n",
    "**On-Policy vs Off-Policy:**\n",
    "- `epochs_per_rollout_batch`: 1 for on-policy GRPO, >=1 if using off-policy GRPO\n",
    "- `train_batch_size`: Rollouts per gradient step (64). For on-policy GRPO, this should be equal to `rollout_batch_size`.\n",
    "\n",
    "**Sampling:**\n",
    "- `sampling_temperature`: Sampling temperature (1.0)\n",
    "- `sampling_max_tokens`: Maximum tokens to sample (2048)\n",
    "- `use_r1_zero_format`: Whether to use R1-Zero format for sampling (True)\n",
    "\n",
    "**Logging & Checkpointing:**\n",
    "- `log_path`: Path to save logs and checkpoints (tmp_logging/grpo-tinker-tutorial)\n",
    "- `save_every`: Save checkpoint every N steps\n",
    "- `eval_every`: Evaluate every N steps\n",
    "\n",
    "### Example Configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chz.chz\n",
    "class GRPOConfig:\n",
    "    \"\"\"Configuration for GRPO training.\"\"\"\n",
    "\n",
    "    # Model & Service\n",
    "    model_name: str = \"Qwen/Qwen3-8B\"\n",
    "    base_url: str | None = None\n",
    "    lora_rank: int = 32\n",
    "\n",
    "    # Training hyperparameters\n",
    "    n_grpo_steps: int = 60\n",
    "    learning_rate: float = 1e-5\n",
    "    rollout_batch_size: int = 256  # Total rollouts (questions = rollout_batch_size // group_size)\n",
    "    group_size: int = 8  # Samples per question\n",
    "\n",
    "    # Off-policy training (for efficiency)\n",
    "    epochs_per_rollout_batch: int = 1  # On-policy=1, off-policy>1 (e.g., 2-4)\n",
    "    train_batch_size: int = 64  # Rollouts per gradient step\n",
    "\n",
    "    # GRPO-specific\n",
    "    advantage_eps: float = 1e-6\n",
    "    use_std_normalization: bool = True\n",
    "    cliprange: float = 0.2  # PPO-style clipping\n",
    "\n",
    "    # Sampling\n",
    "    sampling_temperature: float = 1.0\n",
    "    sampling_max_tokens: int = 2048\n",
    "    use_r1_zero_format: bool = True  # <think></think><answer></answer>\n",
    "\n",
    "    # Logging & Checkpointing\n",
    "    log_path: str = \"tmp_logging/grpo-tinker-tutorial\"\n",
    "    save_every: int = 6\n",
    "    eval_every: int = 6\n",
    "\n",
    "# Create config\n",
    "config = GRPOConfig()\n",
    "\n",
    "# Validate hyperparameters\n",
    "assert config.rollout_batch_size % config.group_size == 0, \"rollout_batch_size must be divisible by group_size\"\n",
    "assert config.train_batch_size <= config.rollout_batch_size, \"train_batch_size must be <= rollout_batch_size\"\n",
    "assert config.rollout_batch_size % config.train_batch_size == 0, \"rollout_batch_size must be divisible by train_batch_size\"\n",
    "\n",
    "n_questions = config.rollout_batch_size // config.group_size\n",
    "batches_per_epoch = config.rollout_batch_size // config.train_batch_size\n",
    "total_gradient_steps = config.epochs_per_rollout_batch * batches_per_epoch\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Questions per step: {n_questions}\")\n",
    "print(f\"  Rollouts per question: {config.group_size}\")\n",
    "print(f\"  Total rollouts per step: {config.rollout_batch_size}\")\n",
    "print(f\"  Train batch size: {config.train_batch_size}\")\n",
    "print(f\"  Batches per epoch: {batches_per_epoch}\")\n",
    "print(f\"  Epochs per rollout batch: {config.epochs_per_rollout_batch}\")\n",
    "print(f\"  Total gradient steps per GRPO step: {total_gradient_steps}\")\n",
    "print(f\"\\n  Mode: {'On-policy' if config.epochs_per_rollout_batch == 1 else 'Off-policy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Loading (GSM8K)\n",
    "\n",
    "As mentioned in the assignment PDF, MATH dataset is not publically available due to copyright issue, so we are\n",
    "using GSM8K dataset as the alternative training dataset.\n",
    "\n",
    "### GSM8K Overview\n",
    "GSM8K is a dataset of grade-school math word problems with numerical answers. It's ideal for testing reasoning and reinforcement learning because:\n",
    "- Clear ground truth (numerical answers)\n",
    "- Requires multi-step reasoning\n",
    "- Automatic reward evaluation\n",
    "\n",
    "### Format\n",
    "Each example has:\n",
    "- `question`: Math word problem\n",
    "- `answer`: Solution with final answer marked as `#### 42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gsm8k_dataset():\n",
    "    \"\"\"Load GSM8K from HuggingFace.\"\"\"\n",
    "    print(\"Loading GSM8K dataset...\")\n",
    "    dataset = datasets.load_dataset(\"openai/gsm8k\", \"main\")\n",
    "    assert isinstance(dataset, datasets.DatasetDict)\n",
    "    train_data = dataset[\"train\"]\n",
    "    test_data = dataset[\"test\"]\n",
    "    print(f\"  Train: {len(train_data)} examples\")\n",
    "    print(f\"  Test: {len(test_data)} examples\")\n",
    "    return train_data, test_data\n",
    "\n",
    "train_dataset, test_dataset = load_gsm8k_dataset()\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample question:\")\n",
    "print(train_dataset[0][\"question\"])\n",
    "print(\"\\nGround truth answer:\")\n",
    "print(train_dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Reward Function\n",
    "\n",
    "The reward function evaluates how good a model's response is. For GSM8K:\n",
    "- **Format reward** (20%): Is the answer properly formatted?\n",
    "- **Answer reward** (80%): Is the numerical answer correct?\n",
    "\n",
    "We try multiple methods to extract the answer from the model's response, which is a combination of\n",
    "reasoning content and the final answer:\n",
    "1. `<answer>...</answer>` tags (R1-Zero format)\n",
    "2. `\\boxed{...}` (LaTeX format)\n",
    "3. Last number in response (fallback)\n",
    "\n",
    "We provide the reward function below, you can skip reading this section, which is not directly\n",
    "related to learning how GRPO works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsm8k_reward_fn(response: str, ground_truth: str) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute reward for GSM8K responses.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: \"reward\", \"format_reward\", \"answer_reward\"\n",
    "    \"\"\"\n",
    "    # Extract ground truth answer (after ####)\n",
    "    gt_match = re.search(r\"####\\s*(.+)$\", ground_truth.strip())\n",
    "    if not gt_match:\n",
    "        return {\"reward\": 0.0, \"format_reward\": 0.0, \"answer_reward\": 0.0}\n",
    "\n",
    "    gt_answer = gt_match.group(1).strip()\n",
    "\n",
    "    # Try multiple extraction methods\n",
    "    # 1. <answer>...</answer> tags\n",
    "    answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.DOTALL)\n",
    "    if answer_match:\n",
    "        model_answer = answer_match.group(1).strip()\n",
    "        format_reward = 1.0\n",
    "    # 2. \\\\boxed{}\n",
    "    elif \"\\\\boxed{\" in response:\n",
    "        boxed_match = re.search(r\"\\\\boxed\\{([^}]+)\\}\", response)\n",
    "        model_answer = boxed_match.group(1).strip() if boxed_match else \"\"\n",
    "        format_reward = 1.0 if boxed_match else 0.0\n",
    "    else:\n",
    "        # Fallback: last number\n",
    "        numbers = re.findall(r\"-?\\d+\\.?\\d*\", response)\n",
    "        model_answer = numbers[-1] if numbers else \"\"\n",
    "        format_reward = 0.5 if numbers else 0.0\n",
    "\n",
    "    # Grade numerical answer\n",
    "    try:\n",
    "        model_num = float(model_answer.replace(\",\", \"\"))\n",
    "        gt_num = float(gt_answer.replace(\",\", \"\"))\n",
    "        answer_reward = 1.0 if abs(model_num - gt_num) < 1e-4 else 0.0\n",
    "    except ValueError:\n",
    "        # String comparison fallback\n",
    "        answer_reward = 1.0 if model_answer == gt_answer else 0.0\n",
    "\n",
    "    # Combined reward (format 20%, answer 80%)\n",
    "    reward = format_reward * 0.2 + answer_reward * 0.8\n",
    "\n",
    "    return {\n",
    "        \"reward\": reward,\n",
    "        \"format_reward\": format_reward,\n",
    "        \"answer_reward\": answer_reward,\n",
    "    }\n",
    "\n",
    "# Test reward function\n",
    "print(\"Testing reward function:\")\n",
    "print(gsm8k_reward_fn(\"<answer>42</answer>\", \"#### 42\"))  # Perfect\n",
    "print(gsm8k_reward_fn(\"The answer is 42\", \"#### 42\"))    # Partial (format)\n",
    "print(gsm8k_reward_fn(\"<answer>43</answer>\", \"#### 42\")) # Wrong answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prompt Building\n",
    "\n",
    "### Why R1-Zero Format?\n",
    "We use the prompt format from DeepSeek R1 Zero, which is `<think></think><answer></answer>`. This format encourages:\n",
    "- Explicit reasoning traces (thinking step)\n",
    "- Separate final answer (makes extraction easier)\n",
    "\n",
    "Inspired by DeepSeek-R1's reinforcement learning from reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gsm8k_prompt(question: str, use_r1_zero_format: bool = True) -> str:\n",
    "    \"\"\"Build prompt for GSM8K question.\"\"\"\n",
    "    if use_r1_zero_format:\n",
    "        return f\"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively.\n",
    "\n",
    "User: {question}\n",
    "Assistant: <think>\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Solve the following math problem. Provide your final numerical answer inside \\\\boxed{{}}.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Solution:\"\"\"\n",
    "\n",
    "# Show example prompt\n",
    "example_question = train_dataset[0][\"question\"]\n",
    "print(\"Example prompt:\")\n",
    "print(build_gsm8k_prompt(example_question)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training Setup\n",
    "\n",
    "Now let's set up the training configs, which includes:\n",
    "\n",
    "- Pick up our base model.\n",
    "- Load the tokenizer.\n",
    "- Apply LoRA to the model for efficient training purpose.\n",
    "- Configure the optimizer.\n",
    "- Configure the sampling params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up Tinker clients...\")\n",
    "service_client = tinker.ServiceClient(base_url=config.base_url)\n",
    "\n",
    "# Check for existing checkpoint to resume\n",
    "resume_info = checkpoint_utils.get_last_checkpoint(config.log_path)\n",
    "if resume_info:\n",
    "    training_client = service_client.create_training_client_from_state(\n",
    "        resume_info[\"state_path\"]\n",
    "    )\n",
    "    start_step = resume_info.get(\"loop_state\", {}).get(\"grpo_step\", 0)\n",
    "    print(f\"  Resuming from GRPO step {start_step}\")\n",
    "else:\n",
    "    # Create fresh training client with LoRA\n",
    "    training_client = service_client.create_lora_training_client(\n",
    "        base_model=config.model_name,\n",
    "        rank=config.lora_rank\n",
    "    )\n",
    "    start_step = 0\n",
    "    print(f\"  Starting fresh training with model: {config.model_name}\")\n",
    "\n",
    "# Setup optimizer (AdamW)\n",
    "adam_params = tinker.types.AdamParams(\n",
    "    learning_rate=config.learning_rate,\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Sampling configuration\n",
    "stop_sequences = [\"</answer>\", \"\\n\\n\\n\"] if config.use_r1_zero_format else [\"\\n\\n\\n\"]\n",
    "sampling_params = tinker.types.SamplingParams(\n",
    "    max_tokens=config.sampling_max_tokens,\n",
    "    temperature=config.sampling_temperature,\n",
    "    stop=stop_sequences,\n",
    ")\n",
    "\n",
    "# Get tokenizer for encoding/decoding\n",
    "print(\"  Loading tokenizer...\")\n",
    "tokenizer = training_client.get_tokenizer()\n",
    "\n",
    "print(\"✓ Tinker setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Main GRPO Training Loop\n",
    "\n",
    "### High-Level Flow\n",
    "\n",
    "```\n",
    "FOR each GRPO step:\n",
    "  1. Sample questions from dataset\n",
    "  2. Save current policy weights → create sampling_client\n",
    "  3. Generate rollouts (group_size per question) [CLOUD]\n",
    "  4. Compute rewards (local)\n",
    "  5. Compute group-normalized advantages (local)\n",
    "  6. Build training datums with old_log_probs\n",
    "  7. Off-policy training loop:\n",
    "       FOR each epoch:\n",
    "         FOR each batch:\n",
    "           - Tinker computes NEW log_probs from current policy\n",
    "           - Compares vs old_log_probs → compute ratio\n",
    "           - GRPO-Clip loss with advantages\n",
    "           - Gradient step\n",
    "  8. Log metrics, save checkpoint, validate\n",
    "```\n",
    "\n",
    "### Key Difference: On-Policy vs Off-Policy\n",
    "\n",
    "**On-Policy** (`epochs_per_rollout_batch=1`):\n",
    "- Single pass through rollout batch\n",
    "- Policy πθ ≈ πθ_old (minimal divergence)\n",
    "- Sample efficient but slower training\n",
    "\n",
    "**Off-Policy** (`epochs_per_rollout_batch>1`):\n",
    "- Multiple passes through rollout batch\n",
    "- Policy πθ diverges from πθ_old\n",
    "- Ratio πθ/πθ_old with clipping prevents instability\n",
    "- More gradient steps per sample → faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop setup\n",
    "n_questions_per_step = config.rollout_batch_size // config.group_size\n",
    "\n",
    "print(f\"Starting GRPO training for {config.n_grpo_steps} steps...\")\n",
    "print(f\"  Rollout batch size: {config.rollout_batch_size} ({n_questions_per_step} questions × {config.group_size} samples)\")\n",
    "if config.epochs_per_rollout_batch == 1:\n",
    "    print(\"  Mode: On-policy (1 gradient step per rollout batch)\")\n",
    "else:\n",
    "    print(f\"  Mode: Off-policy ({config.epochs_per_rollout_batch} epochs per rollout batch)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Sample Questions from Dataset\n",
    "\n",
    "We cycle through the training dataset, shuffling when we wrap around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be inside the main loop - shown here as example for one step\n",
    "grpo_step = start_step\n",
    "t_start = time.time()\n",
    "\n",
    "# Sample batch of questions\n",
    "batch_start = (grpo_step * n_questions_per_step) % len(train_dataset)\n",
    "if batch_start == 0 and grpo_step > 0:\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "    print(\"  Shuffled dataset for new epoch\")\n",
    "\n",
    "batch_end = min(batch_start + n_questions_per_step, len(train_dataset))\n",
    "batch_rows = train_dataset.select(range(batch_start, batch_end))\n",
    "actual_n_questions = len(batch_rows)\n",
    "\n",
    "print(f\"Step {grpo_step}/{config.n_grpo_steps}: Processing {actual_n_questions} questions ({actual_n_questions * config.group_size} rollouts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Save Policy & Create Sampling Client\n",
    "\n",
    "### Why?\n",
    "For **on-policy** GRPO, we need rollouts from the current policy. By saving weights and creating a sampling client, we ensure:\n",
    "- Rollouts come from πθ (not an outdated policy)\n",
    "- Log probabilities are correctly captured\n",
    "\n",
    "### How Tinker Handles This:\n",
    "```python\n",
    "# 1. Save current policy weights to cloud\n",
    "sampling_path = training_client.save_weights_for_sampler().result().path\n",
    "# Returns: tinker://uuid/weights/step_000001\n",
    "\n",
    "# 2. Create sampling client from saved weights\n",
    "sampling_client = service_client.create_sampling_client(model_path=sampling_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_save_start = time.time()\n",
    "\n",
    "# Save current policy for sampling (uploads to Tinker cloud)\n",
    "sampling_path = (\n",
    "    training_client.save_weights_for_sampler(name=f\"step_{grpo_step:06d}\")\n",
    "    .result()\n",
    "    .path\n",
    ")\n",
    "print(f\"  Saved weights: {sampling_path}\")\n",
    "\n",
    "# Create sampling client from saved weights\n",
    "sampling_client = service_client.create_sampling_client(model_path=sampling_path)\n",
    "\n",
    "print(f\"  Save weights time: {time.time() - t_save_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generate Rollouts\n",
    "\n",
    "We sample multiple responses per question (group_size=8) for:\n",
    "- **Group normalization**: Compare responses to the same question\n",
    "- **Variance reduction**: More stable advantage estimates\n",
    "\n",
    "### How Tinker Handles This:\n",
    "Tinker's `sample()` with `num_samples=8` distributes sampling across nodes:\n",
    "```python\n",
    "future = sampling_client.sample(\n",
    "    prompt=tinker.types.ModelInput.from_ints(prompt_tokens),\n",
    "    num_samples=8,  # Sample 8 times in parallel!\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "```\n",
    "\n",
    "Returns `sequence.tokens` and `sequence.logprobs` for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample_start = time.time()\n",
    "\n",
    "# Prepare prompts and launch sampling\n",
    "batch_futures = []\n",
    "batch_prompts = []\n",
    "batch_answers = []\n",
    "\n",
    "for question, answer in zip(batch_rows[\"question\"], batch_rows[\"answer\"]):\n",
    "    prompt = build_gsm8k_prompt(question, config.use_r1_zero_format)\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    batch_prompts.append(prompt_tokens)\n",
    "    batch_answers.append(answer)\n",
    "\n",
    "    # Sample group_size responses per question (distributed across Tinker nodes)\n",
    "    future = sampling_client.sample(\n",
    "        prompt=tinker.types.ModelInput.from_ints(prompt_tokens),\n",
    "        num_samples=config.group_size,  # 8 samples in parallel\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    batch_futures.append(future)\n",
    "\n",
    "print(f\"  Launched {len(batch_futures)} sampling jobs ({config.group_size} samples each)\")\n",
    "\n",
    "# Collect rollout results\n",
    "rollout_responses = []\n",
    "rollout_ground_truths = []\n",
    "rollout_prompt_tokens = []\n",
    "rollout_response_tokens = []\n",
    "rollout_logprobs = []  # OLD log probs from sampling (frozen reference)\n",
    "\n",
    "for prompt_tokens, answer, future in zip(batch_prompts, batch_answers, batch_futures):\n",
    "    sample_result = future.result()\n",
    "\n",
    "    # Process each sample in the group\n",
    "    for sequence in sample_result.sequences:\n",
    "        sampled_tokens = sequence.tokens\n",
    "        sampled_logprobs = sequence.logprobs  # These are OLD log probs (πθ_old)!\n",
    "\n",
    "        if sampled_logprobs is None:\n",
    "            print(\"    Warning: No logprobs returned! Skipping sample.\")\n",
    "            continue\n",
    "\n",
    "        # Decode response\n",
    "        response_text = tokenizer.decode(sampled_tokens, skip_special_tokens=False)\n",
    "\n",
    "        rollout_responses.append(response_text)\n",
    "        rollout_ground_truths.append(answer)\n",
    "        rollout_prompt_tokens.append(prompt_tokens)\n",
    "        rollout_response_tokens.append(sampled_tokens)\n",
    "        rollout_logprobs.append(sampled_logprobs)  # Store for off-policy training\n",
    "\n",
    "n_rollouts = len(rollout_responses)\n",
    "print(f\"  Generated {n_rollouts} rollouts in {time.time() - t_sample_start:.2f}s\")\n",
    "print(f\"  Example response: {rollout_responses[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Compute Rewards & Advantages\n",
    "\n",
    "### Why Group Normalization?\n",
    "GRPO normalizes rewards **within each group** (responses to the same question):\n",
    "```\n",
    "advantage = (reward - mean(group_rewards)) / (std(group_rewards) + eps)\n",
    "```\n",
    "\n",
    "Note: When we compute GRPO loss with Tinker, \"ppo\" loss + subtracting the mean of group rewards is equivalent to\n",
    "GRPO loss.\n",
    "\n",
    "\n",
    "Benefits:\n",
    "- **Relative comparison**: \"This response is better than average for this question\"\n",
    "- **Variance reduction**: Less sensitive to reward scale\n",
    "- **Credit assignment**: Rewards differences, not absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_reward_start = time.time()\n",
    "\n",
    "# Compute rewards and group-normalized advantages\n",
    "# Using the group normalization function defined earlier\n",
    "advantages, raw_rewards, reward_metadata = run_compute_group_normalized_rewards(\n",
    "    reward_fn=gsm8k_reward_fn,\n",
    "    rollout_responses=rollout_responses,\n",
    "    repeated_ground_truths=rollout_ground_truths,\n",
    "    group_size=config.group_size,\n",
    "    advantage_eps=config.advantage_eps,\n",
    "    normalize_by_std=config.use_std_normalization,\n",
    ")\n",
    "\n",
    "print(f\"  Computed rewards in {time.time() - t_reward_start:.2f}s\")\n",
    "print(\"  Reward stats:\")\n",
    "print(f\"    Mean: {reward_metadata['mean_reward']:.3f}\")\n",
    "print(f\"    Std:  {reward_metadata['std_reward']:.3f}\")\n",
    "print(f\"    Min:  {reward_metadata['min_reward']:.3f}\")\n",
    "print(f\"    Max:  {reward_metadata['max_reward']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Build Training Datums\n",
    "\n",
    "We construct `tinker.types.Datum` objects that contain:\n",
    "1. **`model_input`**: Input tokens for forward pass\n",
    "2. **`loss_fn_inputs`**:\n",
    "   - `target_tokens`: What the model should predict\n",
    "   - `advantages`: RL signal (positive=reinforce, negative=discourage)\n",
    "   - `logprobs`: **OLD log probs** from sampling (frozen reference πθ_old)\n",
    "\n",
    "### Causal LM Shift\n",
    "```python\n",
    "full_tokens = prompt + response  # [1, 2, 3, 4, 5]\n",
    "input_tokens = full_tokens[:-1]  # [1, 2, 3, 4] - predict next token\n",
    "target_tokens = full_tokens[1:]  # [2, 3, 4, 5] - what to predict\n",
    "```\n",
    "\n",
    "### Advantage Masking\n",
    "Only compute loss on **response tokens** (not prompt):\n",
    "```python\n",
    "all_advantages = [0.0] * (prompt_len - 1) + [advantage] * len(response_tokens)\n",
    "```\n",
    "\n",
    "### Off-Policy Key: OLD Log Probs\n",
    "The `logprobs` in `loss_fn_inputs` are from **sampling** (πθ_old). These are **frozen** and reused across multiple gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_build_data_start = time.time()\n",
    "training_datums = []\n",
    "skipped_samples = 0\n",
    "\n",
    "for idx, (prompt_tokens, response_tokens, old_logprobs, advantage) in enumerate(\n",
    "    zip(\n",
    "        rollout_prompt_tokens,\n",
    "        rollout_response_tokens,\n",
    "        rollout_logprobs,  # OLD log probs from sampling (πθ_old)\n",
    "        advantages,\n",
    "    )\n",
    "):\n",
    "    if len(response_tokens) == 0:\n",
    "        skipped_samples += 1\n",
    "        continue\n",
    "\n",
    "    # Construct full sequence\n",
    "    full_tokens = prompt_tokens + response_tokens\n",
    "    input_tokens = full_tokens[:-1]  # Shift for causal LM\n",
    "    target_tokens = full_tokens[1:]\n",
    "\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    # Advantage masking: 0 for prompt, advantage for response\n",
    "    all_advantages = [0.0] * (prompt_len - 1) + [advantage.item()] * len(response_tokens)\n",
    "\n",
    "    # OLD log probs: 0 for prompt, actual logprobs for response\n",
    "    all_logprobs = [0.0] * (prompt_len - 1) + old_logprobs\n",
    "\n",
    "    # Validate lengths\n",
    "    if not (len(input_tokens) == len(target_tokens) == len(all_advantages) == len(all_logprobs)):\n",
    "        print(f\"    Warning: Length mismatch at sample {idx}, skipping\")\n",
    "        skipped_samples += 1\n",
    "        continue\n",
    "\n",
    "    # Create Datum for PPO loss\n",
    "    datum = tinker.types.Datum(\n",
    "        model_input=tinker.types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs={\n",
    "            \"target_tokens\": TensorData.from_torch(torch.tensor(target_tokens)),\n",
    "            \"advantages\": TensorData.from_torch(torch.tensor(all_advantages)),\n",
    "            \"logprobs\": TensorData.from_torch(torch.tensor(all_logprobs)),  # OLD log probs!\n",
    "        },\n",
    "    )\n",
    "    training_datums.append(datum)\n",
    "\n",
    "n_training_samples = len(training_datums)\n",
    "print(f\"  Built {n_training_samples} training datums in {time.time() - t_build_data_start:.2f}s\")\n",
    "print(f\"  Skipped {skipped_samples} samples\")\n",
    "\n",
    "if n_training_samples == 0:\n",
    "    print(\"  ERROR: No valid training samples! Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Off-Policy Training Loop\n",
    "\n",
    "### The Core of Off-Policy GRPO!\n",
    "\n",
    "**What Tinker Does Internally (Each Gradient Step)**:\n",
    "1. Forward pass through CURRENT policy πθ with `model_input` tokens\n",
    "2. Compute NEW log probs from current model outputs\n",
    "3. Compare with OLD log probs from `loss_fn_inputs[\"logprobs\"]`\n",
    "4. Compute ratio: `ratio = exp(new_logprobs - old_logprobs) = πθ / πθ_old`\n",
    "5. Apply clipping: `clipped_ratio = clip(ratio, 0.8, 1.2)`\n",
    "6. GRPO-Clip loss: `-min(advantages × ratio, advantages × clipped_ratio)`\n",
    "7. Backward pass\n",
    "\n",
    "### Why This Works Off-Policy:\n",
    "\n",
    "After the first gradient step, πθ ≠ πθ_old, but we keep using the same frozen `old_logprobs`!\n",
    "\n",
    "The **ratio** πθ/πθ_old measures policy divergence:\n",
    "- ratio ≈ 1.0: Policy hasn't changed much (safe)\n",
    "- ratio >> 1.0: Policy much more likely (clipped to 1.2!)\n",
    "- ratio << 1.0: Policy much less likely (clipped to 0.8!)\n",
    "\n",
    "**Clipping** prevents excessive policy updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_start = time.time()\n",
    "\n",
    "# Configure PPO clipping thresholds\n",
    "clip_low = 1.0 - config.cliprange   # 0.8\n",
    "clip_high = 1.0 + config.cliprange  # 1.2\n",
    "\n",
    "# Calculate training schedule\n",
    "n_batches_per_epoch = len(training_datums) // config.train_batch_size\n",
    "n_total_updates = config.epochs_per_rollout_batch * n_batches_per_epoch\n",
    "\n",
    "print(f\"  Training: {config.epochs_per_rollout_batch} epochs, {n_batches_per_epoch} batches per epoch, {n_total_updates} total gradient steps\")\n",
    "\n",
    "# Outer loop: epochs (Off-policy: multiple passes through data)\n",
    "for epoch in range(config.epochs_per_rollout_batch):\n",
    "    # Shuffle datums each epoch to prevent overfitting\n",
    "    shuffled_datums = training_datums.copy()\n",
    "    random.shuffle(shuffled_datums)\n",
    "\n",
    "    # Inner loop: batches (Split rollouts into smaller batches)\n",
    "    for batch_idx in range(n_batches_per_epoch):\n",
    "        # Extract batch of datums\n",
    "        start_idx = batch_idx * config.train_batch_size\n",
    "        end_idx = start_idx + config.train_batch_size\n",
    "        batch_datums = shuffled_datums[start_idx:end_idx]\n",
    "\n",
    "        # Tinker's forward_backward does:\n",
    "        # 1. Forward pass through CURRENT policy → NEW log probs\n",
    "        # 2. Compare NEW vs OLD log probs from loss_fn_inputs[\"logprobs\"]\n",
    "        # 3. Compute ratio = exp(new_logprobs - old_logprobs)\n",
    "        # 4. Apply GRPO-Clip loss with advantages\n",
    "        # 5. Backward pass\n",
    "        fwd_bwd_future = training_client.forward_backward(\n",
    "            batch_datums,  # Contains OLD log probs from sampling!\n",
    "            loss_fn=\"ppo\",  # GRPO-Clip (PPO-style clipping)\n",
    "            loss_fn_config={\n",
    "                \"clip_low_threshold\": clip_low,\n",
    "                \"clip_high_threshold\": clip_high,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Optimizer step (updates policy parameters)\n",
    "        optim_step_future = training_client.optim_step(adam_params)\n",
    "\n",
    "        # Wait for completion\n",
    "        fwd_bwd_result = fwd_bwd_future.result()\n",
    "        optim_result = optim_step_future.result()\n",
    "\n",
    "        if (batch_idx + 1) % max(1, n_batches_per_epoch // 2) == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{config.epochs_per_rollout_batch}, Batch {batch_idx+1}/{n_batches_per_epoch}\")\n",
    "\n",
    "print(f\"  Training completed in {time.time() - t_train_start:.2f}s\")\n",
    "print(f\"  Total gradient steps: {n_total_updates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Validation\n",
    "\n",
    "We need to periodically evaluate on the validation set to check how the training performs because\n",
    "the a converged loss may not imply a good model.\n",
    "\n",
    "### Key: Greedy Sampling\n",
    "Use `temperature=0.0` for deterministic evaluation (no randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_test_set(\n",
    "    sampling_client,\n",
    "    test_dataset,\n",
    "    tokenizer,\n",
    "    use_r1_zero_format: bool,\n",
    "    n_samples: int = 40,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Evaluate model on GSM8K test set.\"\"\"\n",
    "    print(f\"  Running validation on {n_samples} test examples...\")\n",
    "    test_sample = test_dataset.shuffle().select(range(min(n_samples, len(test_dataset))))\n",
    "\n",
    "    correct = 0\n",
    "    format_correct = 0\n",
    "\n",
    "    for question, answer in zip(test_sample[\"question\"], test_sample[\"answer\"]):\n",
    "        prompt = build_gsm8k_prompt(question, use_r1_zero_format)\n",
    "\n",
    "        try:\n",
    "            prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "            response = sampling_client.sample(\n",
    "                prompt=tinker.types.ModelInput.from_ints(prompt_tokens),\n",
    "                num_samples=1,\n",
    "                sampling_params=tinker.types.SamplingParams(\n",
    "                    max_tokens=2048,\n",
    "                    temperature=0.0,  # Greedy for evaluation\n",
    "                    stop=[\"</answer>\", \"\\n\\n\\n\"] if use_r1_zero_format else [\"\\n\\n\\n\"],\n",
    "                ),\n",
    "            ).result()\n",
    "\n",
    "            response_tokens = response.sequences[0].tokens\n",
    "            response_text = tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "            reward_dict = gsm8k_reward_fn(response_text, answer)\n",
    "\n",
    "            if reward_dict[\"answer_reward\"] == 1.0:\n",
    "                correct += 1\n",
    "            if reward_dict[\"format_reward\"] == 1.0:\n",
    "                format_correct += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Validation error: {e}\")\n",
    "            continue\n",
    "\n",
    "    accuracy = correct / n_samples\n",
    "    format_accuracy = format_correct / n_samples\n",
    "\n",
    "    print(f\"  Validation - Accuracy: {accuracy:.1%}, Format: {format_accuracy:.1%}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"format_accuracy\": format_accuracy,\n",
    "    }\n",
    "\n",
    "# Run validation (example - would be inside training loop)\n",
    "if grpo_step % config.eval_every == 0:\n",
    "    try:\n",
    "        val_metrics = validate_on_test_set(\n",
    "            sampling_client,\n",
    "            test_dataset,\n",
    "            tokenizer,\n",
    "            config.use_r1_zero_format,\n",
    "            n_samples=40,\n",
    "        )\n",
    "        print(f\"  Validation metrics: {val_metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Checkpointing\n",
    "\n",
    "### Why?\n",
    "Save training progress to:\n",
    "- Resume if interrupted\n",
    "- Recover best model\n",
    "- Track training history\n",
    "\n",
    "### How Tinker Handles This:\n",
    "\n",
    "Tinker stores checkpoints in **cloud storage** (not local disk!):\n",
    "\n",
    "**Locally** (`log_path/checkpoints.jsonl`):\n",
    "```json\n",
    "{\n",
    "  \"name\": \"grpo_step_000005\",\n",
    "  \"grpo_step\": 6,\n",
    "  \"state_path\": \"tinker://uuid/weights/grpo_step_000005\"\n",
    "}\n",
    "```\n",
    "\n",
    "**In Tinker cloud**: Actual model weights (GBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint (example - would be inside training loop)\n",
    "if (grpo_step + 1) % config.save_every == 0:\n",
    "    print(f\"  Saving checkpoint at step {grpo_step}...\")\n",
    "    checkpoint_utils.save_checkpoint(\n",
    "        training_client=training_client,\n",
    "        name=f\"grpo_step_{grpo_step:06d}\",\n",
    "        log_path=config.log_path,\n",
    "        kind=\"both\",  # Save both training state and sampler weights\n",
    "        loop_state={\"grpo_step\": grpo_step + 1},\n",
    "    )\n",
    "    print(\"  ✓ Checkpoint saved to Tinker cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary: On-Policy vs Off-Policy GRPO\n",
    "\n",
    "### On-Policy GRPO\n",
    "**Config**: `epochs_per_rollout_batch=1`, `train_batch_size=rollout_batch_size`\n",
    "\n",
    "**Flow**:\n",
    "1. Sample 256 rollouts from current policy πθ\n",
    "2. Compute advantages\n",
    "3. **One** gradient step on all 256 rollouts\n",
    "4. Policy updated: πθ → πθ'\n",
    "5. Repeat (sample from πθ' next time)\n",
    "\n",
    "**Pros**: Stable, policy ≈ sampling policy  \n",
    "**Cons**: Sample inefficient (need new rollouts every step)\n",
    "\n",
    "### Off-Policy GRPO\n",
    "**Config**: `epochs_per_rollout_batch=4`, `train_batch_size=64`\n",
    "\n",
    "**Flow**:\n",
    "1. Sample 256 rollouts from current policy πθ_old\n",
    "2. Compute advantages, freeze old_log_probs\n",
    "3. **16 gradient steps** (4 epochs × 4 batches):\n",
    "   - Epoch 1, Batch 1: πθ → πθ₁ (ratio πθ₁/πθ_old ≈ 1.0)\n",
    "   - Epoch 1, Batch 2: πθ₁ → πθ₂ (ratio πθ₂/πθ_old ≈ 1.05)\n",
    "   - ...\n",
    "   - Epoch 4, Batch 4: πθ₁₅ → πθ₁₆ (ratio πθ₁₆/πθ_old ≈ 1.15, clipped to 1.2!)\n",
    "4. Policy updated: πθ_old → πθ₁₆\n",
    "5. Repeat (sample from πθ₁₆ next time)\n",
    "\n",
    "**Pros**: Sample efficient (16 gradient steps per rollout batch)  \n",
    "**Cons**: Policy divergence (mitigated by clipping)\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Off-policy works because:\n",
    "- **Frozen old_log_probs**: Reference from sampling policy πθ_old\n",
    "- **Fresh new_log_probs**: Computed at each gradient step from current πθ\n",
    "- **Ratio clipping**: Prevents excessive divergence (0.8 ≤ πθ/πθ_old ≤ 1.2)\n",
    "\n",
    "This allows us to take **multiple gradient steps** per rollout batch while staying reasonably close to the sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Full Training Loop\n",
    "\n",
    "Now let's put it all together in a complete training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training loop\n",
    "n_questions_per_step = config.rollout_batch_size // config.group_size\n",
    "\n",
    "for grpo_step in range(start_step, config.n_grpo_steps):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GRPO Step {grpo_step}/{config.n_grpo_steps}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Sample questions from dataset\n",
    "    batch_start = (grpo_step * n_questions_per_step) % len(train_dataset)\n",
    "    if batch_start == 0 and grpo_step > 0:\n",
    "        train_dataset = train_dataset.shuffle()\n",
    "        print(\"  Shuffled dataset for new epoch\")\n",
    "    \n",
    "    batch_end = min(batch_start + n_questions_per_step, len(train_dataset))\n",
    "    batch_rows = train_dataset.select(range(batch_start, batch_end))\n",
    "    actual_n_questions = len(batch_rows)\n",
    "    \n",
    "    print(f\"  Processing {actual_n_questions} questions ({actual_n_questions * config.group_size} rollouts)\")\n",
    "    \n",
    "    # 2. Save policy & create sampling client\n",
    "    t_save_start = time.time()\n",
    "    sampling_path = (\n",
    "        training_client.save_weights_for_sampler(name=f\"step_{grpo_step:06d}\")\n",
    "        .result()\n",
    "        .path\n",
    "    )\n",
    "    sampling_client = service_client.create_sampling_client(model_path=sampling_path)\n",
    "    print(f\"  Saved weights in {time.time() - t_save_start:.2f}s\")\n",
    "    \n",
    "    # 3. Generate rollouts\n",
    "    t_sample_start = time.time()\n",
    "    batch_futures = []\n",
    "    batch_prompts = []\n",
    "    batch_answers = []\n",
    "    \n",
    "    for question, answer in zip(batch_rows[\"question\"], batch_rows[\"answer\"]):\n",
    "        prompt = build_gsm8k_prompt(question, config.use_r1_zero_format)\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        batch_prompts.append(prompt_tokens)\n",
    "        batch_answers.append(answer)\n",
    "        \n",
    "        future = sampling_client.sample(\n",
    "            prompt=tinker.types.ModelInput.from_ints(prompt_tokens),\n",
    "            num_samples=config.group_size,\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "        batch_futures.append(future)\n",
    "    \n",
    "    # Collect rollouts\n",
    "    rollout_responses = []\n",
    "    rollout_ground_truths = []\n",
    "    rollout_prompt_tokens = []\n",
    "    rollout_response_tokens = []\n",
    "    rollout_logprobs = []\n",
    "    \n",
    "    for prompt_tokens, answer, future in zip(batch_prompts, batch_answers, batch_futures):\n",
    "        sample_result = future.result()\n",
    "        \n",
    "        for sequence in sample_result.sequences:\n",
    "            sampled_tokens = sequence.tokens\n",
    "            sampled_logprobs = sequence.logprobs\n",
    "            \n",
    "            if sampled_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            response_text = tokenizer.decode(sampled_tokens, skip_special_tokens=False)\n",
    "            \n",
    "            rollout_responses.append(response_text)\n",
    "            rollout_ground_truths.append(answer)\n",
    "            rollout_prompt_tokens.append(prompt_tokens)\n",
    "            rollout_response_tokens.append(sampled_tokens)\n",
    "            rollout_logprobs.append(sampled_logprobs)\n",
    "    \n",
    "    n_rollouts = len(rollout_responses)\n",
    "    print(f\"  Generated {n_rollouts} rollouts in {time.time() - t_sample_start:.2f}s\")\n",
    "    \n",
    "    # 4. Compute rewards & advantages\n",
    "    t_reward_start = time.time()\n",
    "    advantages, raw_rewards, reward_metadata = run_compute_group_normalized_rewards(\n",
    "        reward_fn=gsm8k_reward_fn,\n",
    "        rollout_responses=rollout_responses,\n",
    "        repeated_ground_truths=rollout_ground_truths,\n",
    "        group_size=config.group_size,\n",
    "        advantage_eps=config.advantage_eps,\n",
    "        normalize_by_std=config.use_std_normalization,\n",
    "    )\n",
    "    print(f\"  Rewards: mean={reward_metadata['mean_reward']:.3f}, std={reward_metadata['std_reward']:.3f}\")\n",
    "    \n",
    "    # 5. Build training datums\n",
    "    t_build_start = time.time()\n",
    "    training_datums = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for prompt_tokens, response_tokens, old_logprobs, advantage in zip(\n",
    "        rollout_prompt_tokens, rollout_response_tokens, rollout_logprobs, advantages\n",
    "    ):\n",
    "        if len(response_tokens) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        full_tokens = prompt_tokens + response_tokens\n",
    "        input_tokens = full_tokens[:-1]\n",
    "        target_tokens = full_tokens[1:]\n",
    "        prompt_len = len(prompt_tokens)\n",
    "        \n",
    "        all_advantages = [0.0] * (prompt_len - 1) + [advantage.item()] * len(response_tokens)\n",
    "        all_logprobs = [0.0] * (prompt_len - 1) + old_logprobs\n",
    "        \n",
    "        if not (len(input_tokens) == len(target_tokens) == len(all_advantages) == len(all_logprobs)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        datum = tinker.types.Datum(\n",
    "            model_input=tinker.types.ModelInput.from_ints(tokens=input_tokens),\n",
    "            loss_fn_inputs={\n",
    "                \"target_tokens\": TensorData.from_torch(torch.tensor(target_tokens)),\n",
    "                \"advantages\": TensorData.from_torch(torch.tensor(all_advantages)),\n",
    "                \"logprobs\": TensorData.from_torch(torch.tensor(all_logprobs)),\n",
    "            },\n",
    "        )\n",
    "        training_datums.append(datum)\n",
    "    \n",
    "    print(f\"  Built {len(training_datums)} datums ({skipped} skipped)\")\n",
    "    \n",
    "    if len(training_datums) == 0:\n",
    "        print(\"  ERROR: No valid training samples! Skipping this step.\")\n",
    "        continue\n",
    "    \n",
    "    # 6. Off-policy training loop\n",
    "    t_train_start = time.time()\n",
    "    clip_low = 1.0 - config.cliprange\n",
    "    clip_high = 1.0 + config.cliprange\n",
    "    n_batches_per_epoch = len(training_datums) // config.train_batch_size\n",
    "    n_total_updates = config.epochs_per_rollout_batch * n_batches_per_epoch\n",
    "    \n",
    "    for epoch in range(config.epochs_per_rollout_batch):\n",
    "        shuffled_datums = training_datums.copy()\n",
    "        random.shuffle(shuffled_datums)\n",
    "        \n",
    "        for batch_idx in range(n_batches_per_epoch):\n",
    "            start_idx = batch_idx * config.train_batch_size\n",
    "            end_idx = start_idx + config.train_batch_size\n",
    "            batch_datums = shuffled_datums[start_idx:end_idx]\n",
    "            \n",
    "            fwd_bwd_future = training_client.forward_backward(\n",
    "                batch_datums,\n",
    "                loss_fn=\"ppo\",\n",
    "                loss_fn_config={\n",
    "                    \"clip_low_threshold\": clip_low,\n",
    "                    \"clip_high_threshold\": clip_high,\n",
    "                },\n",
    "            )\n",
    "            optim_step_future = training_client.optim_step(adam_params)\n",
    "            \n",
    "            fwd_bwd_result = fwd_bwd_future.result()\n",
    "            optim_result = optim_step_future.result()\n",
    "    \n",
    "    print(f\"  Training: {n_total_updates} gradient steps in {time.time() - t_train_start:.2f}s\")\n",
    "    \n",
    "    # 7. Validation\n",
    "    if grpo_step % config.eval_every == 0:\n",
    "        try:\n",
    "            val_metrics = validate_on_test_set(\n",
    "                sampling_client, test_dataset, tokenizer, \n",
    "                config.use_r1_zero_format, n_samples=40\n",
    "            )\n",
    "            print(f\"  Validation: accuracy={val_metrics['accuracy']:.1%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Validation failed: {e}\")\n",
    "    \n",
    "    # 8. Checkpointing\n",
    "    if (grpo_step + 1) % config.save_every == 0:\n",
    "        print(f\"  Saving checkpoint...\")\n",
    "        checkpoint_utils.save_checkpoint(\n",
    "            training_client=training_client,\n",
    "            name=f\"grpo_step_{grpo_step:06d}\",\n",
    "            log_path=config.log_path,\n",
    "            kind=\"both\",\n",
    "            loop_state={\"grpo_step\": grpo_step + 1},\n",
    "        )\n",
    "    \n",
    "    print(f\"  Step {grpo_step} completed in {time.time() - t_start:.1f}s\")\n",
    "\n",
    "# Final checkpoint\n",
    "print(\"\\nTraining complete! Saving final checkpoint...\")\n",
    "checkpoint_utils.save_checkpoint(\n",
    "    training_client=training_client,\n",
    "    name=\"final\",\n",
    "    log_path=config.log_path,\n",
    "    kind=\"both\",\n",
    "    loop_state={\"grpo_step\": config.n_grpo_steps},\n",
    ")\n",
    "print(\"✓ Training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
